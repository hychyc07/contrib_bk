/**
@ingroup  icub_applications
\defgroup icub_attentionMechanism attentionMechanism

This is the application controls the icub heading based on the attention system proto-object.
See paper <a href="http://www.robotcub.org/misc/review3/05_Orabona_Metta_Sandini.pdf" >Object-based Visual Attention: a Model for a Behaving Robot, Orabona, Metta, Sandini</a> 

\section intro_sec Description

\image html applicationVisualAttention.png

\section dep_sec Dependencies
Assumes \ref icub_yarp::dev::ServerLogpolarFrameGrabber  is running, and that the logPolar images are provided in the form /icub/cam/right/logpolar and /icub/cam/left/logpolar

\section int_sec Instantiated Modules

- \ref icub_colourProcessor "colourProcessor"

- \ref icub_yuvProcessor "yuvProcessor"

- \ref icub_imageProcessor "imageProcessor"

- \ref icub_imageProcessorInterface "imageProcessorInterface"

- \ref icub_saliencyBlobFinder "saliencyBlobFinder"

- \ref icub_saliencyBlobFinderInterface "saliencyBlobFinderInterface"

- \ref icub_selectiveAttentionEngine "SelectiveAttentionEngine"

- \ref icub_selectiveAttentionInterface "SelectiveAttentionInterface"

\section parameters_sec Parameters
none

\section config_sec Configuration Files
In /app/protoObjectVisualAttention/conf are present a series of configurarion files
- colourProcessorLeft.ini
- imageProcessorLeft.ini
- saliencyBlobFinderLeft.ini


File: there is no .sh file


Global application values are:

- none

For robot devices config.sh contains:

- none

For every module config.sh contains:

- none


\section appl_scripts Application Script
\code
Customize and run appConfig-visualAtt_icubLeft.xml.template
\endcode


\section howto How to run the Application
In order to run this application you must customize the specific template (see \ref appl_scripts). Substitute the name of the node with the name of the node 
in your network. In the template the console node referes to a node with a terminal whereas the powernode refers to a node with enought computational power
(not necessarily with a terminal).

Remember that most of the module need IPP Library so be sure that the selected nodes have this installed.

The application is based on one main module \ref icub_saliencyBlobFinder "saliencyBlobFinder" which extracts regions which are likely to be bounded into
object afterward. This module builds as well the saliency map composed by all the blobs represent with an intensity proportional to saliency of the blob.
This image is named colorLP and is provided to the system throught the port image:o. However, the ouput can be choosen to address other purposes and with the
related interface different output can be selected. Read the documentation of the module \ref icub_saliencyBlobFinder "saliencyBlobFinder" for more details about
the alternatives.

The module \ref icub_saliencyBlobFinderInterface "saliencyBlobFinderInterface", once connected to the saliencyBlobFinder module allow the user to tune some
parameters of the algorithm and therefore to adapt the visual attention. In addition, via saliencyBlobFinderInterface, it can be forced to provide a different output:
<ul>
<li> tagged: the image composed by all the blobs filled with a gray scale equal to their list position</li>
<li> watershed: the result of the watershed operation</li>
<li> foveaBlob: the representation of the blob foveated in that moment</li>
<li> colourVQ: image composed by all the blobs filled with quantization colour</li>
<li> meanColour: the composition of all the colour filled with the mean colour of all the pixel which belong to the blob</li>
<li> maxSaliencyBlob: the representation of the max saliency blob</li>
</ul>


However the saliencyBlobFinder module needs some intensity and colour information provided respectively by the following two modules:
<ul>
<li> \ref icub_imageProcessor "imageProcessor"</li>
<li> \ref icub_colourProcessor "colourProcessor" </li>
</ul>

The output of this module can be merged with other saliency maps produced by the following modules:
- \ref icub_yuvProcessor "yuvProcessor"

The different saliency maps produced can reinforce one each other and through the WTA(WINNER TAKE ALL ALGORITHM) the final output can be selected.
This algorithm is implemented in the the module \ref icub_selectiveAttentionEngine "selectiveAttentionEngine" once the different maps are redirected to the 
different map inputs of the module.
This module is provided with an interface \ref icub_selectiveAttentionInterface "selectiveAttentionInterface" which allows the user to set the coefficients
of the linear combination of maps.This helps to tune which map is more relavant and it can eventually be used as feedback.
Infact the selectiveAttentionModule is development in order to receive commands from the higher level and send feedbacks to the lower level of attention.



Remember: if you use this application with the iCubSimulation you need to change a couple of parameters:
<ul>
<li> remove all the tcp connection which do not work on the local network</li>
<li> change all the name in the apoplcia </li>
</ul> 


\author Stephen Hart & Francesco Rea 

Copyright (C) 2008 RobotCub Consortium

CopyPolicy: Released under the terms of the GNU GPL v2.0.

**/
