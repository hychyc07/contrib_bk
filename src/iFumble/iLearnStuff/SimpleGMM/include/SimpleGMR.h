/*
  Copyright (c) 2010 Florent D'halluin , Sylvain Calinon, Eric Sauser
  LASA Lab, EPFL, CH-1015 Lausanne, Switzerland, 
  http://www.calinon.ch, http://lasa.epfl.ch
  
  This has been touched and adapted by djd 20100724 iCub summer school vvv10

  The program is free for non-commercial academic use. 
  Please acknowledge the authors in any academic publications that have 
  made use of this code or part of it. Please use this BibTex reference: 
 
  @article{Calinon07SMC,
  title="On Learning, Representing and Generalizing a Task in a Humanoid 
  Robot",
  author="S. Calinon and F. Guenter and A. Billard",
  journal="IEEE Transactions on Systems, Man and Cybernetics, Part B. 
  Special issue on robot learning by observation, demonstration and 
  imitation",
  year="2007",
  volume="37",
  number="2",
  pages="286--298"
  }
*/

#ifndef GMR_H_
#define GMR_H_

//#include "MathLib.h"
#include "LasaMatrix.h"
#include "LasaVector.h"
//##include <iCub/MathLib.h>

#ifdef USE_MATHLIB_NAMESPACE
namespace MathLib {
#endif

// EM default parameters

#define RANDOMIZE_FOR_NONSINGULARITY 0.00000001f
//#define RANDOMIZE_FOR_NONSINGULARITY DBL_MIN*10

/* variation threshold for log likelihood EM stopping criterion */
#define EM_STOPPING_CRIT  1e-6  
//#define EM_STOPPING_CRIT  1e-7  
/* added variance on covariances matrices at each EM round (to prevent those matrices 
   from becoming not invertibles */
//#define VARIANCE_KEEPER 1e-5     
#define VARIANCE_KEEPER 0     
/* added variance at the end of EM */
#define VARIANCE_KEEPER_FINAL 0

/**
 * \defgroup GMM
 */

/**
 * Class describing a gaussian mixture model
 *   This has been touched and adapted by djd 20100724 iCub summer school vvv10
 * NOTE THAT THE STOPPING CRITERION ONLY CURRENTLY STOPS WHEN THE LIKELIHOOD CHANGES NOT AT ALL - FOR ALIKELIHOOD THAT INCREASES AGAIN, THE BEAHVIOUR IS JUST TO KEEP GOING - SO OFTEN GETS STUCK IN CYCLES - JUST EXHAUSTS MAXITER
 * there is a simple fix but not sure why this stopping criteron was chosen - since the threshold is ignored
 *
 * \ingroup GMM
 * basic usage (training a 5 state model, with the input vectors in the dataset Matrix):
 * 
 * @code 
 * 
 * NewGaussianMixture gmm;
 * gmm.initEM_random(5,dataset);
 * gmm.doEM(dataset);
 * @endcode
 * 
 * The result of a training can be save and retrieve from a file
 * through the saveParams and loadParams methods. And we may perform
 * GMR with the doRegression methods.
 */

class NewGaussianMixture {
  
 public :
  int nState,dim; 
  Matrix mu;
  Matrix *sigma;
  Vector priors;

  NewGaussianMixture(){sigma = NULL;};
  
  /**
  * added by djd 20100728
  */
  ~NewGaussianMixture();
  
  /**
  * added by djd 20100728
  */
  void init_basic(int nState,int dim);

   /**
    * Initialise the model with parameters stored in a file
    *
    * load the different parameters ( priors, means, covariances matrices ..)
    * from a file generated by the saveParams function, or Matlab
    *
    * \param filename file containing the desired GMM description
    */
  bool loadParams(const char filename[]);

  /**
   * save current parameters in a file 
   *
   * save current means. covariances matrices and priors in a text file
   *
   * \param filename filename to save the current model in
   */

  void saveParams(const char filename []);
 
  /**
   *Display current parameters on screen (for debugging purposes)
   */
  void debug(void);
    
  /**
   * Perform Gaussian Mixture regression.
   * 
   * \return a mean vector corresponding to the in vector. 
   * \param inComponents integer vector representing dimension of "in" Vector 
   * \param outComponents integer vector representing dimension of returned Vector 
   * \param SigmaOut will be set to the corresponding covariance Matrix
   */
  Vector doRegression( Vector in,
               Matrix& SigmaOut,
               Vector inComponents,
               Vector outComponents);

  /**
   * Perform regression for several input vectors
   * \return Matrix whose rows are output means
   * \param in Matrix whose row are the inputs vectors 
   */
  Matrix doRegression( Matrix in,
               Matrix * SigmaOut,
               Vector inComponents,
               Vector outComponents);

  /**
   * Perform regression only using a subset of states 
   * given by indexes stored in the states input vector
   */
  
  Vector doRegressionPartial( Vector in,Matrix& Sigma,
                  Vector inComponents,Vector outComponents,
                  Vector states);

  /**
   * Perform Gaussian Mixture regression.
   * 
   * \return a mean vector corresponding to the in vector. 
   * \param inComponents integer vector representing dimension of "in" Vector 
   * \param outComponents integer vector representing dimension of returned Vector 
   * \param SigmaOut will be set to the corresponding covariance Matrix
   */
  Matrix doOffsetRegression( Matrix in,
                             Vector offset,
                             Matrix* SigmaOut,
                             Vector inComponents,
                             Vector outComponents);

  Matrix doOffsetRegression( Matrix in,
                             Vector offset,
                             Matrix* SigmaOut,
                             Vector inComponents,
                             Vector outComponents,
                             Matrix inPrior,
                             Vector priorComponents);

  Vector getRegressionOffset( Vector in, 
                              Vector currOut,
                              Vector inComponents,
                              Vector outComponents);
  Vector getRegressionOffset( Vector in, 
                              Vector currOut,
                              Vector inComponents,
                              Vector outComponents,
                              Vector inPrior,
                              Vector priorComponents);

  /**
   * save results of arbitrary regression 
   * \param fileMu name of file for means matrix
   * \param fileSigma name of file for covariances array
   * \param outData matrix of output means 
   * \param outSigma array of covariances matrices */
 
  bool saveMuAndSigma(const char fileMu[], const char fileSigma[], 
              Matrix outData, Matrix outSigma[]);   
  
  /**
   *Compute probabilty of v ( corresponding to dimension given 
   *in the Components vector) knowing state. 
   */

  double pdfState(Vector v,Vector Components,int state);
  double distState(Vector v,Vector Components,int state);

  /**
   * compute probability of v, knowing state 
   *
   * This function requires calling to inverseSigmaMatrices()
   * first. This call is done When the covariance matrices are
   * computed through the doEM method.
   */
  double pdfState(Vector v,int state);
  
  /**
   * Initialise Expectation Maximization randomly.
   * 
   * \param nState desired number of state 
   * \param Dataset Matrix (nSamples rows, dimension columns) 
   * containing input training set
   * 
   * assign each sample to a random state, and initialize GMM
   * parameters to perform EM
   */
  void initEM_random(int nState,Matrix Dataset);

  /**
   * Initialise Expectation Maximization.  
   *
   * \param nState desired number of state 
   * \param Dataset Matrix (nSamples rows, dimension
   * columns) containing the training set 
   *
   * assign each sample to a state according to its value in the first
   * dimension (time). Reduce the needed number of iteration for EM if
   * the inputs vectors describe time dependant values (like gestures
   * for example ..)
   */
  void initEM_TimeSplit(int nState,Matrix Dataset, int offCount = 1);


  /**
   * Initialise Expectation Maximization using k-means algo
   */
  void initEM_kmeans(int nState,Matrix Dataset);
 
  /**
   * Perform Expectation Maximisation 
   *
   * \param blank_run  run blank_run step without updating
   * covariances, if you init with random methodm this allow a fake
   * k-means before performing EM .. default 0
   *
   * \param max_iter maximum number of iteration of EM loop. The
   * algorithm stop when the log likelihood does not vary of a least
   * epsilon = 10e-1 between two iteration .
   * 
   * \return now returns the log likelhood of the data - good way to check if it failed.
   */

  double doEM(Matrix DataSet,int blank_run=0,int max_iter = 100);
  void doEM(Matrix DataSet, Vector weights, int blank_run=0,int max_iter = 100);

  /**
   * one EM step ..
   * return the log likelihood computed *before* the update step
   */
 
  double stepEM(Matrix Dataset, bool update_covariances=true);
  double stepEM(Matrix Dataset, Vector weights, bool update_covariances=true);

  /**
   * compute an arbitray Gaussian probability
   */
  static long double GaussianPDF(Vector input,Vector mean,Matrix covariance);

  void balancePriors();

  /**
   * likelihood computations
   */ 
  double likelihood(Vector input);
  double log_likelihood(Vector input);
  double log_likelihood(Matrix inputs);

  void addDataPointPartial(const Vector& dataPoint, double lambda, int state, const Vector& inComponents);

  void addDataPoint(const Vector& dataPoint, double lambda, const Vector& inComponents);

  void addDataSet(const Matrix& dataSet, double lambda, const Vector& inComponents); 

 // Vector sampleFrom();
  
  /**
  * Added by djd 20100723
  * For this component, generate the conditional gaussian distribution given that the vector 
  * \param knownVec has been observed for the dimensions observed in
  * \param inComponents -- which is a bit of a misnomer since these are dimensions not components
  * \param newSigma the covariance of the resulting distribution
  * \param newMu the mean of the resulting distribution
  *
  */
  void conditionalCompononent(int compno,Matrix &newSigma, Vector &newMu, Vector &knownVec,Vector& inComponents,Vector& outComponents);
  /**
  * Added by djd 20100723
  * For this component, generate the marginal distribution marginalising out all dimensions except for
  * \param inComponents -- which is a bit of a misnomer since these are dimensions not components
  * \param newSigma the covariance of the resulting distribution
  * \param newMu the mean of the resulting distribution
  *
  */  
  void marginalComponent(int compno,Matrix &newSigma, Vector &newMu, const Vector& outComponents);

  /**
  * Added by djd 20100723
  * Generate the conditional distribution given that the vector 
  * \param knownVec has been observed for the dimensions observed in
  * \param inComponents -- which is a bit of a misnomer since these are dimensions not components
  * \return The Gaussian mixture model that results - OWNERSHIP PASSED TO CALLER
  *
  */
  NewGaussianMixture *conditionalMixture(Vector &knownVec,Vector& inComponents,Vector& outComponents);
  
  /**
  * Added by djd 20100723
  * Find the probability density function of the distribution at v, taking into account component weights, etc.
   */
  double pdf(Vector v);
  
  std::string toStr();

 protected :
 int inverseSigmaMatrices(void);
 int inverseSigmaMatrix(int no);
  Matrix *invSigma;
  double *detSigma;
  double log_lik;
  double log_lik_old;
  /**
  * This function just randomizes the sigma matrices a bit so that we don't get nan and 0 probabilities creeping inline
  * It's a bit of a hack but it'll allow the learner to learn some models where otherwise might've become singular
  */
  void maintainNonsingular();
  void deleteSingularComponents();
  void deleteComponent(int i);
  void deleteDuplicates();
  void fixSigma(int sigind);
  void merge(int i,int j);
  inline void fixSigma(){for(int i=0;i<nState;i++)fixSigma(i);}
};

//std::ostream& operator<<(std::ostream &os, Vector &val);
//std::ostream& operator<<(std::ostream &os, Matrix &val);




#ifdef USE_MATHLIB_NAMESPACE
}
#endif
#endif

